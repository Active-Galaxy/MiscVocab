{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOq4H+WVFx03bgjq6drBeyz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Active-Galaxy/MiscVocab/blob/main/Ask_roBERTa_Anything.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1mvQxENqXobi"
      },
      "source": [
        "\n",
        "# Overview\n",
        "\n",
        "This notebook uses the Haystack question answering system roBERTa to query a private set of documents. roBERTa, a large language model (LLM), built using a system called BERT, is not a conversational AI. It is more like a TV show's portrayal of AI from the '70s (maybe '60s). The input is not very conversational and will it won't handle standard database style questions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Optionally (but highly recomended), an AI generated summary is available using Gemini, the Google LLM. When you <a class=\"button button-primary\" href=\"https://makersuite.google.com/app/apikey\" target=\"_blank\" rel=\"noopener noreferrer\">get a GOOGLE_API_KEY</a> (they are free), check the box below before starting and insert the GOOGLE_API_KEY when prompted. This key is not stored.\n",
        "\n",
        "connect_to_Gemini = True # @param {type:\"boolean\"}\n",
        "GOOGLE_API_KEY = \"XXXXXXXXXXXXXXXXX\"\n",
        "if connect_to_Gemini:\n",
        "  GOOGLE_API_KEY =  input(\"Insert GOOGLE_API_KEY:  \").strip()\n",
        "  if len(GOOGLE_API_KEY) != 39:\n",
        "    print(\"Invalid GOOGLE_API_KEY -- click 'Runtime/Interupt execution' and restart\")\n",
        "\n",
        "from datetime import datetime\n",
        "#print(\"Starting at\", datetime.now().strftime('%H:%M:%S'))\n"
      ],
      "metadata": {
        "id": "hgieQ_WmwJ4t",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Starting the Notebook\n",
        "To start the notebook, click on ***Runtime/Run all***. The notebook takes time to install and instantiate the software on the Google Colab server. Load time is about 3:45 for ***CPU only***. The spining animation and green checkmarks on the left hand side are an indication that something good is happening.\n",
        "\n",
        "Note: Do not click on the black circles with the white triangles or you may have to restart everything.\n",
        "\n",
        "Scroll to the bottom of the screen after starting the notebook."
      ],
      "metadata": {
        "id": "02mAUcCVwFM9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNLeir77Xobi",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Install Haystack\n",
        "%%capture\n",
        "\n",
        "!pip install --upgrade pip\n",
        "print(\"----------------------------------------\")\n",
        "%pip install --root-user-action=ignore farm-haystack[inference,elasticsearch]\n",
        "#!pip install --root-user-action=ignore farm-haystack[colab] # Requires restart"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Connect to Elasticsearch\n",
        "#%%capture\n",
        "\n",
        "#!wget https://github.com/Active-Galaxy/MiscVocab/raw/main/Living%20by%20Faith.haystackDocs.txt -O docs.txt\n",
        "\n",
        "#  https://docs.haystack.deepset.ai/docs/document_store\n",
        "# Here, we're using the ElasticsearchDocumentStore which connects to a running Elasticsearch service.\n",
        "# It's a fast and scalable text-focused storage option.\n",
        "# This service runs independently from Haystack and persists even after the Haystack program has finished running.\n",
        "\n",
        "%%bash\n",
        "\n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "chown -R daemon:daemon elasticsearch-7.9.2\n"
      ],
      "metadata": {
        "id": "otrmrLxQv5u_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N78hLiaXobj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Start Elasticsearch Server\n",
        "%%bash --bg\n",
        "\n",
        "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEz--14xXobj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Wait for server\n",
        "\n",
        "import subprocess\n",
        "def has_gpu():\n",
        "  try:\n",
        "    subprocess.check_output('nvidia-smi')\n",
        "    print('GPU detected')\n",
        "    return True\n",
        "  except Exception: # this command not being found can raise quite a few different errors depending on the configuration\n",
        "    print('CPU Only')\n",
        "    return False\n",
        "\n",
        "wait30 = not has_gpu()\n",
        "print(\"Waiting for server to start...\")\n",
        "import time\n",
        "\n",
        "time.sleep(30)\n",
        "\n",
        "import os\n",
        "from haystack.document_stores import ElasticsearchDocumentStore\n",
        "\n",
        "# Get the host where Elasticsearch is running, default to localhost\n",
        "host = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\n",
        "\n",
        "if wait30:\n",
        "  print(\"Extra wait...\")\n",
        "  time.sleep(30)\n",
        "\n",
        "document_store = ElasticsearchDocumentStore(host=host, username=\"\", password=\"\", index=\"document\")\n",
        "# ElasticsearchDocumentStore is up and running and ready to store the Documents.\n",
        "print(\"Server started.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load Documents\n",
        "%%capture\n",
        "!wget https://github.com/Active-Galaxy/MiscVocab/raw/main/Living%20by%20Faith.haystackDocs.txt -O docs.txt\n",
        "\n",
        "from haystack import Document\n",
        "docs = []\n",
        "\n",
        "doc_count = 0\n",
        "found = False\n",
        "with open('docs.txt', 'r') as file:\n",
        "  for line in file:\n",
        "    try:\n",
        "      c,m = line.strip().split('\\t')\n",
        "    except:\n",
        "      if not found:\n",
        "        print(\"Read Error at line\", doc_count, line)\n",
        "        found = True\n",
        "      continue\n",
        "\n",
        "    d = Document(c, meta={'source': m}, id= \"roBERTa\"+str(doc_count))\n",
        "    doc_count+=1\n",
        "    docs.append(d)#'''\n",
        "\n",
        "document_store.write_documents(docs)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2IyjQ5ag2MUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UenZQ94oXobk",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Create querying_pipeline\n",
        "%%capture\n",
        "\n",
        "# https://docs.haystack.deepset.ai/docs/retriever\n",
        "from haystack.nodes import BM25Retriever\n",
        "retriever = BM25Retriever(document_store=document_store)\n",
        "\n",
        "# https://huggingface.co/deepset/roberta-base-squad2\n",
        "# https://docs.haystack.deepset.ai/docs/reader#models\n",
        "# https://haystack.deepset.ai/tutorials/02_finetune_a_model_on_your_data\n",
        "\n",
        "from haystack.nodes import FARMReader\n",
        "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\n",
        "\n",
        "from haystack import Pipeline\n",
        "querying_pipeline = Pipeline()\n",
        "querying_pipeline.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
        "querying_pipeline.add_node(component=reader, name=\"Reader\", inputs=[\"Retriever\"])\n",
        "\n",
        "import textwrap\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "model = None\n",
        "GOOGLE_API_KEY = GOOGLE_API_KEY.strip()\n",
        "if len(GOOGLE_API_KEY) == 39:\n",
        "  !pip install -q -U --root-user-action=ignore google-generativeai\n",
        "  import google.generativeai as genai\n",
        "  try:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "def queryDocs(query):\n",
        "  prediction = querying_pipeline.run(query=query,\n",
        "      params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 10}})\n",
        "\n",
        "  return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Asking a Question\n",
        "*While you are waiting for the server to start...*\n",
        "\n",
        "When the software is ready for you, a long thin box will appear below along with the prompt \"*Type query text or 'exit':*\". Type your query into the box and hit ***enter***. The query does not have to be in the form of a question. A single word or phrase can also be used.\n",
        "\n",
        "A query can show up to 5 responses from roBERTa. Each has a *score*, the extracted *answer*, the document *source*, and the *context* in which the answer was found. If you are using a GOOGLE_API_KEY, the Gemini LLM will return a summary based on the highest scoring document sections.\n",
        "\n",
        "The input box will reappear after printing the current set of answers.\n",
        "\n",
        "To start, try some of these queries:\n",
        "- What is the first question?\n",
        "- Who is Jesus Christ?\n",
        "- What is the mystery of the gospel?\n",
        "- What is the mystery of iniquity?\n",
        "- How do I obtain perfection?\n",
        "- I feel that I am trying too hard to find Jesus.\n",
        "- I have trouble being humble\n",
        "\n",
        "When finished with this notebook, click ***Runtime/Disconnect and delete runtime*** to be a good citizen.\n",
        "\n"
      ],
      "metadata": {
        "id": "jXnlYSoWaO0F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB9_XBlGXobr",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Query loop (Keep Closed ❯)\n",
        "# https://docs.haystack.deepset.ai/docs/optimization#choosing-the-right-top-k-values\n",
        "# https://docs.haystack.deepset.ai/docs/pipelines#arguments\n",
        "# https://haystack.deepset.ai/tutorials/24_building_chat_app\n",
        "# https://docs.haystack.deepset.ai/docs/agent\n",
        "\n",
        "# https://colab.research.google.com/github/deepset-ai/haystack-tutorials/blob/main/tutorials/22_Pipeline_with_PromptNode.ipynb#scrollTo=f6NFmpjEO-qb\n",
        "\n",
        "print(doc_count, \"document sections read.\")\n",
        "\n",
        "if model is None and connect_to_Gemini:\n",
        "  print(\"Invalid GOOGLE_API_KEY\")\n",
        "\n",
        "from pprint import pprint\n",
        "from haystack.utils import print_answers\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "last_docs = \"\"\n",
        "print(\"The query loop is available.\")\n",
        "while True:\n",
        "  query = input(\"\\nType query text or 'exit' then hit enter:\\n\").strip()\n",
        "  if query == \"exit\":\n",
        "      break\n",
        "\n",
        "  prediction = queryDocs(query)\n",
        "\n",
        "  max_score = 0\n",
        "  for i in range(len(prediction[\"answers\"])):\n",
        "    score = prediction[\"answers\"][i].score\n",
        "    max_score = max(max_score, score)\n",
        "\n",
        "  max_score = round(max_score, 2) * 100\n",
        "  if max_score < 1:\n",
        "    print(\"(No search results)\")\n",
        "    continue\n",
        "\n",
        "  #print(\"Max score:\", max_score, \"%\")\n",
        "  current_docs = \"\"\n",
        "  doc_ids = []\n",
        "  for i in range(len(prediction[\"answers\"])):\n",
        "    answer = prediction[\"answers\"][i]\n",
        "    score = round(answer.score, 2) * 100\n",
        "    if score < 1 :\n",
        "      continue\n",
        "\n",
        "    score = str(int(score)) + \"%\"\n",
        "    simple = answer.answer.replace('\\\\', '')\n",
        "    sents = simple.split(\".\")\n",
        "    if len(sents) > 1:\n",
        "      simple = sents[0] + \".\"\n",
        "    elif not simple.endswith(\".\"):\n",
        "      simple += \".\"\n",
        "\n",
        "    simple = simple[0].upper() + simple[1:]\n",
        "    context = answer.context.replace('\\\\', '')\n",
        "    if i < 5:\n",
        "      print(score,\"\\t\",simple,\"\\n\\t\\t\", answer.meta, \"\\n\\t\\t\",\"Context:\",context)\n",
        "      doc_ids.append(answer.document_ids[0])\n",
        "\n",
        "    current_docs += context + \"; \"\n",
        "\n",
        "  if model is not None:\n",
        "    prompt = \"Summarize the following related text for the given question\\n\\n\"\n",
        "    #docs = current_docs + \"; \" + last_docs\n",
        "    last_docs = current_docs\n",
        "    docs = \"\"\n",
        "    for id in doc_ids:\n",
        "      for document in  prediction[\"documents\"]:\n",
        "        if document.id == id:\n",
        "          docs += document.content + \"; \"#'''\n",
        "\n",
        "    prompt += '''Related text: ''' + docs + '''\\n\\n'''\n",
        "    prompt += '''Question: ''' + query + '''\\n\\n'''\n",
        "    #print(prompt)\n",
        "    try:\n",
        "      response = model.generate_content(prompt)\n",
        "      display(to_markdown(\"**Summary**: \"+response.text))\n",
        "    except:\n",
        "      display(to_markdown(\"**Summary**: (Gemini response error)\"))\n",
        "  print()\n",
        "  #pprint(prediction)\n",
        "  #print_answers(prediction, details=\"minimum\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WRQgDtivq2Ai"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}